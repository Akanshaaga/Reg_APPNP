Source Code for "Addressing the Impact of Localized Training Data in Graph Neural Networks."

In this work, we introduce a simple yet effective regularization technique designed to mitigate the distributional shift effect in the latent space, resulting in more accurate results when dealing with biased training data.

We have integrated our proposed regularization technique and reported notable improvements in accuracy across various popular GNN models. Researchers and practitioners can utilize our code for different types of biased data and experiment with different GNN models to address the challenges posed by localized training data.
